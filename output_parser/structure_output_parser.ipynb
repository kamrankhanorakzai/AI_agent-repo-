{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c798a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate,PromptTemplate\n",
    "# from langchain.output_parsers import ResponseSchema,StructuredOutputParser\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49857a",
   "metadata": {},
   "source": [
    "# Output Parsers in LangChain \n",
    "        string output parser\n",
    "        pydantic output parser\n",
    "        jason output parser\n",
    "        csv output parser\n",
    "        structured output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3f1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm=HuggingFaceEndpoint(\n",
    "#     repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "#     task=\"text-generation\",\n",
    "#     # temperature= 0.7\n",
    "#     )\n",
    "\n",
    "# chat=ChatHuggingFace(llm=llm)\n",
    "# print(chat.invoke([\"What is the capital of France?\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c20b690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_agent_repo\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 17, 'total_tokens': 25}, 'model_name': 'deepseek-ai/DeepSeek-V3.2', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c7829-e94a-72f2-97f3-450756771617-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 17, 'output_tokens': 8, 'total_tokens': 25}\n"
     ]
    }
   ],
   "source": [
    "llm=HuggingFaceEndpoint(repo_id=\"deepseek-ai/DeepSeek-V3.2\",\n",
    "                        temperature= 1.5\n",
    "                        ,task=\"text-generation\")\n",
    "\n",
    "chat=ChatHuggingFace(llm=llm,)\n",
    "print(chat.invoke(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3af64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black holes are extreme cosmic objects with powerful gravity, formed from collapsed massive stars. They come in types like stellar, supermassive, and intermediate-mass. Advances like the Event Horizon Telescope now make black holes observable.\n"
     ]
    }
   ],
   "source": [
    "# 1st prompt -> detailed report\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 line summary on the following text. /n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = template1 | chat | parser | template2 | chat | parser\n",
    "\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4059bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts_about_black_holes': [{'fact': \"The boundary of a black hole is called the event horizon. Once anything, including light, crosses this point, it cannot escape the black hole's gravitational pull.\"}, {'fact': \"Black holes can vary in size. Stellar black holes form from collapsed stars and can be several times the Sun's mass, while supermassive black holes, found at the centers of galaxies, can contain millions or billions of solar masses.\"}, {'fact': \"Despite their name, black holes are not entirely 'black' due to Hawking radiationâ€”a theoretical process where black holes can emit particles and lose mass over extremely long timescales.\"}, {'fact': \"Black holes distort time and space around them, an effect predicted by Einstein's theory of general relativity known as gravitational time dilation.\"}, {'fact': \"The first-ever image of a black hole's event horizon, captured by the Event Horizon Telescope in 2019, shows the supermassive black hole at the center of the galaxy M87.\"}]}\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | chat | parser\n",
    "\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d019644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=chat\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 about the topic'),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca9fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Asiri Perera' age=25 city='Colombo'\n"
     ]
    }
   ],
   "source": [
    "model=chat\n",
    "class Person(BaseModel):\n",
    "\n",
    "    name: str = Field(description='Name of the person')\n",
    "    age: int = Field(gt=18, description='Age of the person')\n",
    "    city: str = Field(description='Name of the city the person belongs to')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "final_result = chain.invoke({'place':'sri lankan'})\n",
    "\n",
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
