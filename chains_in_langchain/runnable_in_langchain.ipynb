{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ§  1ï¸âƒ£ First: What Problem Was LangChain Facing?\n",
    "\n",
    "# Before Runnables, LangChain had:\n",
    "\n",
    "# * `LLMChain`\n",
    "# * `SequentialChain`\n",
    "# * `SimpleSequentialChain`\n",
    "# * `RouterChain`\n",
    "# * `ConversationChain`\n",
    "# * Custom chains for everythingâ€¦\n",
    "\n",
    "# Each chain had:\n",
    "\n",
    "# * Different input formats\n",
    "# * Different output formats\n",
    "# * Different ways of calling them\n",
    "\n",
    "# ðŸ‘‰ Result:\n",
    "# The library became large, messy, and hard for beginners (like most AI engineers in 2023).\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸš¨ The Core Problem\n",
    "\n",
    "# Every component behaved differently.\n",
    "\n",
    "# Example:\n",
    "\n",
    "# | Component | How You Call It             |\n",
    "# | --------- | --------------------------- |\n",
    "# | LLM       | `.predict()`                |\n",
    "# | Prompt    | `.format()`                 |\n",
    "# | Retriever | `.get_relevant_documents()` |\n",
    "# | Chain     | `.run()`                    |\n",
    "\n",
    "# Nothing was standardized.\n",
    "\n",
    "# So connecting them required glue code.\n",
    "\n",
    "# Thatâ€™s bad software design.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ’¡ The Big Idea: Standard Interface\n",
    "\n",
    "# LangChain developers asked:\n",
    "\n",
    "# > What if EVERYTHING behaved the same way?\n",
    "\n",
    "# So they created:\n",
    "\n",
    "# # ðŸ”¥ Runnable = A Standard Unit of Work\n",
    "\n",
    "# A Runnable is simply:\n",
    "\n",
    "# > Any object that has an `invoke(input)` method.\n",
    "\n",
    "# Thatâ€™s it.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸŽ¯ What That Means\n",
    "\n",
    "# Now:\n",
    "\n",
    "# | Component | How You Call It |\n",
    "# | --------- | --------------- |\n",
    "# | LLM       | `.invoke()`     |\n",
    "# | Prompt    | `.invoke()`     |\n",
    "# | Parser    | `.invoke()`     |\n",
    "# | Retriever | `.invoke()`     |\n",
    "# | Chain     | `.invoke()`     |\n",
    "\n",
    "# Everything behaves the same.\n",
    "\n",
    "# This is called:\n",
    "\n",
    "# > Interface Standardization\n",
    "\n",
    "# Very important software engineering concept.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ§© Why This Is Powerful\n",
    "\n",
    "# Because now you can do this:\n",
    "\n",
    "# ```python\n",
    "# prompt | model | parser\n",
    "# ```\n",
    "\n",
    "# Why does this work?\n",
    "\n",
    "# Because:\n",
    "\n",
    "# * `prompt` is Runnable\n",
    "# * `model` is Runnable\n",
    "# * `parser` is Runnable\n",
    "\n",
    "# They all follow same contract:\n",
    "\n",
    "# ```python\n",
    "# def invoke(input):\n",
    "#     ...\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ— Software Architecture Perspective\n",
    "\n",
    "# This follows:\n",
    "\n",
    "# * OOP principle\n",
    "# * Interface abstraction\n",
    "# * Polymorphism\n",
    "# * Composition over inheritance\n",
    "\n",
    "# You don't need to know what the object is.\n",
    "# You only care:\n",
    "\n",
    "# > Does it implement `invoke()`?\n",
    "\n",
    "# If yes â†’ it can be chained.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ§µ What Happens Internally\n",
    "\n",
    "# When you write:\n",
    "\n",
    "# ```python\n",
    "# chain = prompt | model | parser\n",
    "# ```\n",
    "\n",
    "# LangChain creates something like:\n",
    "\n",
    "# ```python\n",
    "# class RunnableSequence:\n",
    "#     def invoke(self, input):\n",
    "#         output1 = prompt.invoke(input)\n",
    "#         output2 = model.invoke(output1)\n",
    "#         output3 = parser.invoke(output2)\n",
    "#         return output3\n",
    "# ```\n",
    "\n",
    "# So `|` is just syntactic sugar for composition.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ”€ Why RunnableParallel Works\n",
    "\n",
    "# Because Runnable is standardized.\n",
    "\n",
    "# When you write:\n",
    "\n",
    "# ```python\n",
    "# RunnableParallel({\n",
    "#     \"a\": chain1,\n",
    "#     \"b\": chain2\n",
    "# })\n",
    "# ```\n",
    "\n",
    "# LangChain does:\n",
    "\n",
    "# ```python\n",
    "# {\n",
    "#     \"a\": chain1.invoke(input),\n",
    "#     \"b\": chain2.invoke(input)\n",
    "# }\n",
    "# ```\n",
    "\n",
    "# Same interface â†’ easy scaling.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸŒ³ Why RunnableBranch Works\n",
    "\n",
    "# Branch logic:\n",
    "\n",
    "# ```python\n",
    "# if condition(input):\n",
    "#     return runnable1.invoke(input)\n",
    "# else:\n",
    "#     return runnable2.invoke(input)\n",
    "# ```\n",
    "\n",
    "# Again â€” everything is runnable.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ“¦ Why This Fixed LangChain\n",
    "\n",
    "# Before:\n",
    "\n",
    "# * 20 different chain types\n",
    "# * Confusing architecture\n",
    "\n",
    "# After Runnables:\n",
    "\n",
    "# * Everything = Runnable\n",
    "# * Chains are just compositions\n",
    "# * Much smaller core\n",
    "# * Much easier mental model\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ§  The Real Mental Model\n",
    "\n",
    "# Think like this:\n",
    "\n",
    "# > Runnable = function with superpowers\n",
    "\n",
    "# But better than function because it also supports:\n",
    "\n",
    "# * streaming\n",
    "# * async\n",
    "# * batching\n",
    "# * tracing\n",
    "# * retry\n",
    "# * fallback\n",
    "\n",
    "# All standardized.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # ðŸ­ Real Production Impact\n",
    "\n",
    "# Because everything is Runnable:\n",
    "\n",
    "# You can add:\n",
    "\n",
    "# ```python\n",
    "# chain.with_retry()\n",
    "# chain.with_fallback()\n",
    "# chain.with_config()\n",
    "# chain.stream()\n",
    "# chain.batch()\n",
    "# ```\n",
    "\n",
    "# Without rewriting logic.\n",
    "\n",
    "# This is production-level architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f02056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-Specific Runnables (4:42): \n",
    "# These are core LangChain components like PromptTemplate, LLMs, Retrievers, \n",
    "# and Parsers that have been converted into runnables (4:55). \n",
    "# They have a specific purpose, such as helping to design prompts or interact with LLMs (5:15).\n",
    "\n",
    "\n",
    "# Runnable Primitives (6:22):\n",
    "#  These runnables act as fundamental building \n",
    "# blocks that help connect other task-specific runnables to create complex AI workflows (6:27).\n",
    "# Examples:\n",
    "#  RunnableSequence,\n",
    "#  RunnableParallel, \n",
    "#  RunnableBranch,\n",
    "#  RunnableLambda,\n",
    "# RunnablePassthrough . \n",
    "# They orchestrate how different runnables interact, whether sequentially, \n",
    "# in parallel, or conditionally (6:41)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7ï¸âƒ£ With Config / Utility Wrappers\n",
    "\n",
    "# These are enhancements you can apply to any Runnable:\n",
    "\n",
    "# with_retry()\n",
    "\n",
    "# Retry if model fails.\n",
    "\n",
    "# with_fallback()\n",
    "\n",
    "# Fallback to another model if first fails.\n",
    "\n",
    "# batch()\n",
    "\n",
    "# Process multiple inputs at once.\n",
    "\n",
    "# stream()\n",
    "\n",
    "# Stream tokens instead of full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71225bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "chain = RunnableSequence(prompt1, model, parser, prompt2, model, parser)\n",
    "\n",
    "print(chain.invoke({'topic':'AI'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Explain the following joke - {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'explanation': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'cricket'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Generate a tweet about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Generate a Linkedin post about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'tweet': RunnableSequence(prompt1, model, parser),\n",
    "    'linkedin': RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({'topic':'AI'})\n",
    "\n",
    "print(result['tweet'])\n",
    "print(result['linkedin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e978719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='Write a joke about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "joke_gen_chain = RunnableSequence(prompt, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    'joke': RunnablePassthrough(),\n",
    "    'word_count': RunnableLambda(word_count)\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_gen_chain, parallel_chain)\n",
    "\n",
    "result = final_chain.invoke({'topic':'AI'})\n",
    "\n",
    "final_result = \"\"\"{} \\n word count - {}\"\"\".format(result['joke'], result['word_count'])\n",
    "\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableBranch, RunnableLambda\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    template='Summarize the following text \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "report_gen_chain = prompt1 | model | parser\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split())>300, prompt2 | model | parser),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "final_chain = RunnableSequence(report_gen_chain, branch_chain)\n",
    "\n",
    "print(final_chain.invoke({'topic':'Russia vs Ukraine'}))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
