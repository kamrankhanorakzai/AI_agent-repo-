{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6fa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from prompts import create_prompt\n",
    "load_dotenv()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "model=GoogleGenerativeAI(model=\"gemini-2.5-flash\",temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc7cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper \"GPT-3: Language Models are Few-Shot Learners\" introduces GPT-3, a groundbreaking language model demonstrating an impressive ability called \"few-shot learning.\" Traditionally, AI models need extensive re-training (fine-tuning) with hundreds or thousands of specific examples to perform a new task, like translating text or answering questions. GPT-3, however, can learn new tasks with *very few* examples – sometimes just one, or even zero – simply by being given instructions or a couple of demonstrations within the input prompt itself, without altering its core programming. Think of it like a super-smart student who has read every book imaginable (its massive pre-training). When asked to solve a new type of math problem, they don't need to go back to school for a new course; they can quickly grasp the pattern and solve it after seeing just one or two examples.\n",
      "\n",
      "This remarkable capability stems from its immense scale (175 billion parameters) and being pre-trained on a colossal amount of internet text, where its primary job was to predict the next word in a sequence. This extensive training allows it to build a deep understanding of language, facts, and reasoning. At its core, GPT-3's continuous learning involves calculating the probability of the next word given all the words that came before it. Mathematically, it's constantly solving for:\n",
      "\n",
      "$P(\\text{next word} \\mid \\text{previous words})$\n",
      "\n",
      "This means \"the probability of a specific next word, given the context of previous words.\" For instance, if you feed it \"The capital of France is\", the model calculates probabilities for words like \"Paris\", \"London\", \"Berlin\", etc., aiming to assign the highest probability to \"Paris\" based on its training. A simplified way to imagine this in code is:\n",
      "\n",
      "```python\n",
      "# Imagine GPT-3's internal thought process for prediction\n",
      "context = \"The capital of France is\"\n",
      "# Model computes probabilities for the next word\n",
      "next_word_probabilities = model.predict_next_token(context)\n",
      "# Example output: {'Paris': 0.95, 'London': 0.02, 'Berlin': 0.01, ...}\n",
      "# The model then selects the word with the highest probability.\n",
      "```\n",
      "This ability to infer patterns and apply its vast knowledge within the input context, rather than through explicit re-training, is what defines its few-shot learning breakthrough.\n"
     ]
    }
   ],
   "source": [
    "template =  create_prompt()   \n",
    "chain = template | model\n",
    "result = chain.invoke({\n",
    "'paper_input':\"GPT-3: Language Models are Few-Shot Learners\",\n",
    "'style_input':\"Beginner-Friendly\",\n",
    "'length_input':\"Short (1-2 paragraphs)\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971c014a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an **open-source framework** designed to simplify the creation of applications powered by large language models (LLMs). It acts as a bridge, allowing LLMs to interact with other sources of data and computational tools, extending their capabilities far beyond just generating text.\n",
      "\n",
      "Think of it as the **operating system or a sophisticated toolkit** for building with LLMs. It provides the **building blocks** and the **orchestra conductor** to make LLMs work together with external data and tools to perform complex tasks.\n",
      "\n",
      "## Why does LangChain exist? What problem does it solve?\n",
      "\n",
      "While LLMs are incredibly powerful at generating text, understanding context, and answering questions, they have inherent limitations:\n",
      "\n",
      "1.  **Lack of up-to-date information:** Their knowledge is capped at their training data.\n",
      "2.  **No access to external tools:** They can't browse the internet, run code, interact with APIs, or query databases on their own.\n",
      "3.  **Limited memory:** By default, they don't remember past interactions in a conversation.\n",
      "4.  **Difficulty with complex, multi-step tasks:** Breaking down a complex problem into smaller, manageable steps and executing them often requires careful orchestration.\n",
      "\n",
      "LangChain addresses these limitations by providing a standardized and modular way to:\n",
      "\n",
      "*   **Connect LLMs to external data:** Retrieve relevant information from documents, databases, or the web to augment the LLM's knowledge.\n",
      "*   **Allow LLMs to interact with tools:** Give LLMs the ability to use calculators, search engines, APIs, or custom functions to perform actions or get real-time data.\n",
      "*   **Manage conversation history:** Enable chatbots to have memory and remember previous turns in a conversation.\n",
      "*   **Orchestrate complex workflows:** Chain together multiple LLM calls, tool uses, and data retrievals into coherent, multi-step processes.\n",
      "\n",
      "## Key Concepts and Components of LangChain:\n",
      "\n",
      "LangChain is built around several core modules:\n",
      "\n",
      "1.  **Models:** Integrations with various LLM providers (OpenAI, Hugging Face, Cohere, Anthropic, etc.) and different model types (completion, chat).\n",
      "2.  **Prompts:** Tools for constructing and managing prompts for LLMs, including prompt templates, example selectors, and output parsers.\n",
      "3.  **Chains:** Sequences of calls to LLMs or other utilities. They allow you to combine different components to perform multi-step operations (e.g., summarize a document, then ask a question about the summary).\n",
      "4.  **Retrieval:** Interfaces for interacting with external data. This includes:\n",
      "    *   **Document Loaders:** For loading data from various sources (PDFs, websites, databases).\n",
      "    *   **Text Splitters:** For breaking down large documents into smaller, manageable chunks.\n",
      "    *   **Vector Stores:** For storing and querying embeddings of documents, crucial for \"Retrieval Augmented Generation\" (RAG).\n",
      "    *   **Retrievers:** For fetching relevant documents based on a query.\n",
      "5.  **Agents:** These are the most powerful feature. Agents allow an LLM to dynamically decide which tools to use and in what order, based on the user's input. The LLM acts as a \"reasoning engine\" that can observe, plan, and execute.\n",
      "6.  **Memory:** Mechanisms to persist state between calls of a chain or agent, essential for conversational applications.\n",
      "7.  **Tools:** Functions or APIs that an agent can call to interact with the outside world (e.g., a search engine tool, a calculator tool, a custom API tool).\n",
      "\n",
      "## What can you build with LangChain? (Use Cases)\n",
      "\n",
      "*   **Question Answering over Custom Data (RAG):** Build chatbots that can answer questions based on your specific documents (e.g., company policies, medical records, research papers) by retrieving relevant information before generating a response.\n",
      "*   **Intelligent Chatbots:** Create conversational agents that can remember past interactions, use tools (like a calendar or CRM), and perform actions.\n",
      "*   **Data Analysis & Summarization:** Automate the process of extracting insights or summarizing large volumes of text or structured data.\n",
      "*   **Code Generation & Explanation:** Develop tools that can write code based on natural language descriptions or explain existing code.\n",
      "*   **Autonomous Agents:** Create agents that can break down complex goals into sub-tasks, execute those tasks using various tools, and report back.\n",
      "*   **Connecting to APIs/Databases:** Build applications that allow users to interact with your existing systems using natural language.\n",
      "\n",
      "## Benefits of using LangChain:\n",
      "\n",
      "*   **Enhanced LLM Capabilities:** Extends LLMs beyond their inherent limitations.\n",
      "*   **Modularity and Reusability:** Provides a structured way to build components that can be reused across different applications.\n",
      "*   **Faster Development:** Abstracts away much of the complexity of integrating LLMs with other systems, speeding up development.\n",
      "*   **Flexibility:** Supports a wide range of LLMs, data sources, and tools.\n",
      "*   **Active Community:** A rapidly growing ecosystem and community contribute to its development and offer support.\n",
      "\n",
      "In essence, LangChain empowers developers to move beyond simple LLM prompts and build sophisticated, context-aware, and action-oriented AI applications.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages=[\n",
    "    SystemMessage(content='You are a helpful assistant'),\n",
    "    HumanMessage(content='Tell me about LangChain')\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "messages.append(AIMessage(content=result))\n",
    "\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9034b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your previous chat provided a detailed explanation of **LangChain**, an open-source framework that helps build powerful LLM applications by connecting large language models to external data, tools, and memory for complex, multi-step tasks.\n"
     ]
    }
   ],
   "source": [
    "# Example: Continue conversation\n",
    "messages.append(HumanMessage(content='give me very short summary about my previous chats'))\n",
    "result2 = model.invoke(messages)\n",
    "messages.append(AIMessage(content=result2))\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "414b96fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, the Dusra! A brilliant and often baffling delivery in the world of cricket!\n",
      "\n",
      "In simple terms, it's a special trick ball bowled by an **off-spin bowler**.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1.  **The Usual Off-Spin:** Normally, an off-spinner makes the ball spin *into* a right-handed batsman (from leg-side to off-side). The batsman expects this turn.\n",
      "2.  **The Dusra Deception:** With the **Dusra** (which means \"the second one\" or \"the other one\" in Urdu), the bowler uses an almost identical bowling action – **that's the crucial part!** – but makes the ball either:\n",
      "    *   **Go straight on** instead of turning in.\n",
      "    *   Or even **turn *away* from the right-handed batsman** (from off-side to leg-side), which is the complete opposite of their usual delivery!\n",
      "\n",
      "The magic is in the deception! The batsman sees the same hand and arm action, expects the usual turn, and then the ball does something completely different. This often leads to them misreading the line, missing the ball, and getting bowled, caught behind, or stumped.\n",
      "\n",
      "It was famously perfected and popularized by the fantastic Pakistani spinner, Saqlain Mushtaq. It's a real weapon for any off-spinner who can master it!\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {domain} expert'),\n",
    "    ('human', 'Explain in simple terms, what is {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({'domain':'cricket','topic':'Dusra'})\n",
    "result = model.invoke(prompt)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
